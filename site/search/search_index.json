{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Work Journal","text":"<p>Transform your informal work notes into professional accomplishment summaries using AI.</p>"},{"location":"#what-is-work-journal","title":"What is Work Journal?","text":"<p>Work Journal captures your daily work and transforms it into well-structured documentation. Write informal notes, and AI extracts the key information into professional summaries with impact metrics, collaboration details, and technical achievements.</p> <p>Perfect for: - Performance reviews and promotion packets - 1:1 meeting preparation with managers - Project summaries and retrospectives - Tracking technical growth over time</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83e\udd16 AI-Powered Processing: Structures informal notes into professional summaries</li> <li>\ud83d\udcdd Flexible Input: Inline writing, editor support, or file import</li> <li>\ud83c\udfaf 1:1 Summaries: Generate manager-friendly reports with Top 3 highlights</li> <li>\ud83d\udcca Impact Tracking: Automatic assessment of scope, significance, and business value</li> <li>\ud83c\udff7\ufe0f Smart Organization: Project detection, collaboration tracking, and tagging</li> <li>\ud83d\udcbe Local Storage: All data stays on your machine with soft-delete protection</li> <li>\ud83d\udd27 Multi-LLM Support: Works with OpenAI, Anthropic, local models, and custom endpoints</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Install: <code>uv add work-journal</code></li> <li>Run: <code>work-journal</code></li> <li>Setup: Follow the guided onboarding to configure your AI provider</li> <li>Write: Start adding your daily accomplishments</li> </ol> <p>The application includes comprehensive online help - just press 'h' in any interface!</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation &amp; Setup - Get up and running in minutes</li> <li>Provider Configuration - Set up AI models (local + cloud mixing)</li> <li>Configuration Management - Switch between different setups</li> </ul>"},{"location":"#example-workflow","title":"Example Workflow","text":"<ol> <li>Daily Entry: Record what you accomplished (natural language, any format)</li> <li>AI Processing: System structures your notes into professional summaries</li> <li>Review &amp; Refine: Use online help and AI feedback to perfect entries</li> <li>Generate Summaries: Create 1:1 reports with Top 3 highlights</li> <li>Performance Reviews: Export organized accomplishments for reviews</li> </ol> <p>Simple, powerful, and designed to make work documentation effortless. All data stays local.</p>"},{"location":"configurations/","title":"Configuration Management","text":"<p>Switch between different AI setups for different use cases. Perfect for adapting to changing needs, testing new models, or optimizing costs.</p>"},{"location":"configurations/#what-are-configurations","title":"What are Configurations?","text":"<p>A configuration is a named set of model assignments for the three functions: - Conversation: Interactive chat and entry refinement - Processing: Structuring raw entries into professional summaries - JIRA Matching: Finding relevant tickets for your work</p> <p>You can create multiple configurations and switch between them instantly.</p>"},{"location":"configurations/#common-configuration-patterns","title":"Common Configuration Patterns","text":""},{"location":"configurations/#work-setup","title":"Work Setup","text":"<p>For daily work with good balance of quality and cost:</p> <pre><code>Name: \"work-daily\"\n\u251c\u2500\u2500 Conversation: llama3.1:8b (Ollama) - Fast local feedback\n\u251c\u2500\u2500 Processing: Claude 3.5 Sonnet (Anthropic) - High quality\n\u2514\u2500\u2500 JIRA Matching: llama3.1:8b (Ollama) - Private matching\n</code></pre>"},{"location":"configurations/#review-prep","title":"Review Prep","text":"<p>For performance review preparation with maximum quality:</p> <pre><code>Name: \"review-prep\"\n\u251c\u2500\u2500 Conversation: Claude 3.5 Sonnet (Anthropic) - Best refinement\n\u251c\u2500\u2500 Processing: GPT-4 (OpenAI) - Premium structuring\n\u2514\u2500\u2500 JIRA Matching: GPT-3.5-turbo (OpenAI) - Reliable matching\n</code></pre>"},{"location":"configurations/#budget-mode","title":"Budget Mode","text":"<p>For cost-conscious usage:</p> <pre><code>Name: \"budget\"\n\u251c\u2500\u2500 Conversation: GPT-3.5-turbo (OpenAI) - Affordable cloud\n\u251c\u2500\u2500 Processing: GPT-4 (OpenAI) - Quality where needed\n\u2514\u2500\u2500 JIRA Matching: llama3.1:8b (Ollama) - Free local\n</code></pre>"},{"location":"configurations/#demotesting","title":"Demo/Testing","text":"<p>For testing new models or providers:</p> <pre><code>Name: \"testing\"\n\u251c\u2500\u2500 Conversation: New Model (Test Provider)\n\u251c\u2500\u2500 Processing: Current Best (Reliable Provider)\n\u2514\u2500\u2500 JIRA Matching: Fast Local (Ollama)\n</code></pre>"},{"location":"configurations/#managing-configurations","title":"Managing Configurations","text":""},{"location":"configurations/#view-current-configuration","title":"View Current Configuration","text":"<p>The active configuration is shown in: - System menu \u2192 Model configuration - Main menu status (if configured)</p>"},{"location":"configurations/#switch-configurations","title":"Switch Configurations","text":"<ol> <li>Access System menu (<code>x</code>)</li> <li>Choose Model configuration (<code>1</code>)</li> <li>Select numbered configuration from list</li> <li>Configuration switches immediately</li> </ol>"},{"location":"configurations/#create-new-configuration","title":"Create New Configuration","text":"<ol> <li>Access System menu (<code>x</code>)</li> <li>Choose Model configuration (<code>1</code>)  </li> <li>Select Create New Configuration (<code>1</code>)</li> <li>Follow 4-step guided process:</li> <li>Add providers (if needed)</li> <li>Select models</li> <li>Assign models to functions</li> <li>Name and activate</li> </ol>"},{"location":"configurations/#test-configuration","title":"Test Configuration","text":"<p>Before switching, test your setup: 1. Go to model configuration menu 2. Choose Test Current Setup (<code>t</code>) 3. Verifies all three functions work properly</p>"},{"location":"configurations/#configuration-strategy","title":"Configuration Strategy","text":""},{"location":"configurations/#by-use-case","title":"By Use Case","text":"<p>Daily Work: Optimize for speed and cost - Local conversation for quick feedback - Cloud processing for quality - Local JIRA for privacy</p> <p>Important Documents: Optimize for quality - Best models for both conversation and processing - Accept higher costs for critical work</p> <p>Experimentation: Test safely - Keep one function with reliable model - Test new models in less critical functions</p>"},{"location":"configurations/#by-environment","title":"By Environment","text":"<p>Office/Stable Internet: Use cloud models freely Travel/Unreliable Internet: Prefer local models Secure/Air-gapped: Local models only Cost-Sensitive: Local models with minimal cloud usage</p>"},{"location":"configurations/#example-workflow","title":"Example Workflow","text":"<ol> <li> <p>Start with Hybrid Setup:    <pre><code>daily-work: Local conversation + Cloud processing + Local JIRA\n</code></pre></p> </li> <li> <p>Add Quality Setup for Important Work:    <pre><code>high-quality: Cloud everything with best models\n</code></pre></p> </li> <li> <p>Add Budget Setup for Experimentation:    <pre><code>budget: Cheapest viable models for testing\n</code></pre></p> </li> <li> <p>Switch as Needed:</p> </li> <li>Daily work: Use \"daily-work\"</li> <li>Performance reviews: Switch to \"high-quality\"  </li> <li>Testing features: Switch to \"budget\"</li> </ol>"},{"location":"configurations/#tips-and-best-practices","title":"Tips and Best Practices","text":""},{"location":"configurations/#naming-conventions","title":"Naming Conventions","text":"<p>Use descriptive names that indicate purpose: - <code>work-daily</code>, <code>review-prep</code>, <code>demo-mode</code> - <code>high-quality</code>, <code>budget</code>, <code>local-only</code> - <code>testing-new-models</code>, <code>backup-config</code></p>"},{"location":"configurations/#testing-new-models","title":"Testing New Models","text":"<ol> <li>Create test configuration with new model for one function</li> <li>Keep other functions with known-good models</li> <li>Test thoroughly before switching production config</li> <li>Delete test config when done</li> </ol>"},{"location":"configurations/#backup-configurations","title":"Backup Configurations","text":"<p>Always maintain at least one reliable configuration: - Keep a \"backup\" config with proven models - Don't modify your primary config when experimenting - Test new configurations before making them primary</p>"},{"location":"configurations/#model-function-guidelines","title":"Model Function Guidelines","text":"<ul> <li>Conversation: Prioritize speed and responsiveness</li> <li>Processing: Prioritize quality and accuracy</li> <li>JIRA Matching: Prioritize privacy and cost</li> </ul>"},{"location":"configurations/#configuration-storage","title":"Configuration Storage","text":"<p>Configurations are stored in <code>~/.work-journal/settings.json</code>: - All data is local - Easy to backup and restore - Can be manually edited if needed (advanced users)</p>"},{"location":"configurations/#troubleshooting","title":"Troubleshooting","text":"<p>Configuration Won't Switch:  - Check if all models in target configuration are available - Use \"Test Current Setup\" to verify connectivity - Ensure API keys are set for cloud providers</p> <p>Missing Models: - For local models: Ensure Ollama/LM Studio is running - For cloud models: Verify API keys and account access - Use provider setup to re-add missing models</p> <p>Performance Issues: - Local models: Check system resources (RAM, CPU) - Cloud models: Check internet connectivity - Consider switching to lighter models for problematic functions</p> <p>The built-in help system (<code>h</code> key) provides contextual guidance for all configuration operations!</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get Work Journal up and running in minutes with our guided setup.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code># Install with uv (recommended)\nuv add work-journal\n\n# Or with pip\npip install work-journal\n</code></pre>"},{"location":"getting-started/#first-run","title":"First Run","text":"<p>Launch the application:</p> <pre><code>work-journal\n</code></pre> <p>First-time users will be guided through a 4-step onboarding process:</p> <ol> <li>Add AI Providers - Connect to OpenAI, Anthropic, local models, or custom endpoints</li> <li>Select Models - Choose specific models from your providers  </li> <li>Create Configuration - Assign models to different functions (conversation, processing, JIRA matching)</li> <li>Activate Configuration - Set your new configuration as active</li> </ol>"},{"location":"getting-started/#environment-setup","title":"Environment Setup","text":"<p>Work Journal uses environment variables for API keys. Create a <code>.env</code> file in your project directory or set up global environment variables:</p> <pre><code># For OpenAI\nexport OPENAI_API_KEY=\"your-api-key-here\"\n\n# For Anthropic  \nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\n\n# For custom endpoints (optional)\nexport CUSTOM_API_KEY=\"your-custom-key\"\n</code></pre> <p>Global Setup: The application can also use <code>~/.work-journal/.env</code> for system-wide configuration.</p>"},{"location":"getting-started/#supported-providers","title":"Supported Providers","text":""},{"location":"getting-started/#cloud-providers","title":"Cloud Providers","text":"<ul> <li>OpenAI - GPT-4, GPT-3.5-turbo, and other OpenAI models</li> <li>Anthropic - Claude 3.5 Sonnet, Claude 3.5 Haiku  </li> <li>Custom OpenAI-Compatible - Any service with OpenAI-compatible API</li> </ul>"},{"location":"getting-started/#local-models","title":"Local Models","text":"<ul> <li>Ollama - Run models locally with privacy</li> <li>LM Studio - User-friendly local model interface</li> <li>Custom Local - Any OpenAI-compatible local service</li> </ul>"},{"location":"getting-started/#quick-configuration-examples","title":"Quick Configuration Examples","text":""},{"location":"getting-started/#cloud-only-setup","title":"Cloud-Only Setup","text":"<p>Perfect for getting started quickly with high-quality results:</p> <pre><code>Providers: OpenAI + Anthropic\nConfiguration:\n- Conversation: Claude 3.5 Sonnet (interactive refinement)\n- Processing: GPT-4 (entry structuring) \n- JIRA Matching: GPT-3.5-turbo (fast ticket matching)\n</code></pre>"},{"location":"getting-started/#privacy-first-setup","title":"Privacy-First Setup","text":"<p>All processing stays on your machine:</p> <pre><code>Providers: Ollama\nConfiguration:\n- Conversation: llama3.1:8b\n- Processing: llama3.1:8b\n- JIRA Matching: llama3.1:8b\n</code></pre>"},{"location":"getting-started/#hybrid-setup-recommended","title":"Hybrid Setup (Recommended)","text":"<p>Mix local and cloud for optimal cost/performance:</p> <pre><code>Providers: Anthropic + Ollama\nConfiguration:\n- Conversation: llama3.1:8b (local, fast feedback)\n- Processing: Claude 3.5 Sonnet (cloud, high quality)\n- JIRA Matching: llama3.1:8b (local, privacy)\n</code></pre>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>Work Journal includes comprehensive online help throughout the application:</p> <ul> <li>Press 'h' in any interface to toggle contextual help</li> <li>Help content is specific to each screen and function</li> <li>All major features have detailed guidance and examples</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Once installed and configured:</p> <ol> <li>Add your first entry - Use the 'a' option from the main menu</li> <li>Explore the interface - Use 'h' for help in any screen</li> <li>Try 1:1 summaries - Generate professional reports with 's'</li> <li>Configure entities - Manage collaborators, projects, and tags in System menu ('x')</li> </ol> <p>The application is designed to be self-explanatory with its built-in help system!</p>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":"<p>Connection Issues: Use System menu \u2192 Model configuration \u2192 Test connection to verify your setup.</p> <p>Missing API Keys: Check your environment variables and <code>.env</code> file configuration.</p> <p>Local Models: Ensure Ollama or LM Studio is running before testing connections.</p> <p>For detailed configuration options, see Provider Configuration and Configuration Management.</p>"},{"location":"providers/","title":"Provider Configuration","text":"<p>Set up AI providers to power your Work Journal. Mix local and cloud models for optimal cost, performance, and privacy.</p>"},{"location":"providers/#provider-types","title":"Provider Types","text":""},{"location":"providers/#cloud-providers","title":"Cloud Providers","text":"<p>OpenAI <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre> - Models: GPT-4, GPT-3.5-turbo, GPT-4-turbo - Best for: High-quality processing, reliable results - Cost: Pay per token</p> <p>Anthropic <pre><code>export ANTHROPIC_API_KEY=\"your-api-key\"\n</code></pre> - Models: Claude 3.5 Sonnet, Claude 3.5 Haiku - Best for: Excellent reasoning, conversation - Cost: Pay per token</p>"},{"location":"providers/#local-models","title":"Local Models","text":"<p>Ollama (Recommended for local) <pre><code># Install Ollama first\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Pull a model\nollama pull llama3.1:8b\n\n# Ollama runs on http://localhost:11434/v1\n</code></pre> - Models: Llama, CodeLlama, Mistral, and more - Best for: Privacy, no API costs, fast local processing - Cost: Free (after hardware)</p> <p>LM Studio <pre><code># Download and start LM Studio\n# Configure to run on http://localhost:1234/v1  \n</code></pre> - Models: Wide selection with user-friendly interface - Best for: Easy local model management - Cost: Free (after hardware)</p>"},{"location":"providers/#custom-openai-compatible-endpoints","title":"Custom OpenAI-Compatible Endpoints","text":"<p>Any service that implements the OpenAI API format: <pre><code>export CUSTOM_API_KEY=\"your-api-key\"  # if needed\n</code></pre></p>"},{"location":"providers/#recommended-configurations","title":"Recommended Configurations","text":""},{"location":"providers/#starter-cloud-only","title":"Starter: Cloud-Only","text":"<p>Best for: Getting started quickly, maximum quality</p> <pre><code>Configuration: \"cloud-quality\"\n\u251c\u2500\u2500 Conversation: Claude 3.5 Sonnet (Anthropic)\n\u251c\u2500\u2500 Processing: GPT-4 (OpenAI)  \n\u2514\u2500\u2500 JIRA Matching: GPT-3.5-turbo (OpenAI)\n</code></pre> <p>Pros: Highest quality results, minimal setup Cons: Ongoing API costs, requires internet</p>"},{"location":"providers/#privacy-local-only","title":"Privacy: Local-Only","text":"<p>Best for: Air-gapped environments, complete privacy</p> <pre><code>Configuration: \"local-privacy\"\n\u251c\u2500\u2500 Conversation: llama3.1:8b (Ollama)\n\u251c\u2500\u2500 Processing: llama3.1:8b (Ollama)\n\u2514\u2500\u2500 JIRA Matching: llama3.1:8b (Ollama)\n</code></pre> <p>Pros: Complete privacy, no API costs, works offline Cons: Requires capable hardware, lower quality than cloud</p>"},{"location":"providers/#hybrid-mixed-models-recommended","title":"Hybrid: Mixed Models (Recommended)","text":"<p>Best for: Optimal cost/performance balance</p> <pre><code>Configuration: \"hybrid-optimal\"  \n\u251c\u2500\u2500 Conversation: llama3.1:8b (Ollama) - Fast local feedback\n\u251c\u2500\u2500 Processing: Claude 3.5 Sonnet (Anthropic) - High-quality structuring\n\u2514\u2500\u2500 JIRA Matching: llama3.1:8b (Ollama) - Private ticket matching\n</code></pre> <p>Pros: Cost-effective, good quality, private for sensitive operations Cons: Requires local setup + cloud accounts</p>"},{"location":"providers/#budget-cost-optimized","title":"Budget: Cost-Optimized","text":"<p>Best for: Minimizing cloud costs while maintaining quality</p> <pre><code>Configuration: \"budget-smart\"\n\u251c\u2500\u2500 Conversation: GPT-3.5-turbo (OpenAI) - Affordable cloud\n\u251c\u2500\u2500 Processing: GPT-4 (OpenAI) - Quality where it matters  \n\u2514\u2500\u2500 JIRA Matching: llama3.1:8b (Ollama) - Free local matching\n</code></pre>"},{"location":"providers/#model-function-guide","title":"Model Function Guide","text":""},{"location":"providers/#conversation-model","title":"Conversation Model","text":"<ul> <li>Purpose: Interactive refinement, follow-up questions</li> <li>Usage: Moderate (user-driven)</li> <li>Recommendation: Fast, responsive models (local or GPT-3.5-turbo)</li> </ul>"},{"location":"providers/#processing-model","title":"Processing Model","text":"<ul> <li>Purpose: Structure raw entries into professional summaries</li> <li>Usage: Heavy (every entry)</li> <li>Recommendation: Highest quality models (GPT-4, Claude 3.5 Sonnet)</li> </ul>"},{"location":"providers/#jira-matching-model","title":"JIRA Matching Model","text":"<ul> <li>Purpose: Find relevant tickets for your work</li> <li>Usage: Light (when enabled)</li> <li>Recommendation: Fast, cheap models (local or GPT-3.5-turbo)</li> </ul>"},{"location":"providers/#setup-process","title":"Setup Process","text":"<ol> <li> <p>Install Local Models (if using):    <pre><code># For Ollama\nollama pull llama3.1:8b\nollama pull codellama:7b  # Optional for code-focused work\n</code></pre></p> </li> <li> <p>Set Environment Variables:    <pre><code># Add to ~/.work-journal/.env or local .env\nOPENAI_API_KEY=your_key_here\nANTHROPIC_API_KEY=your_key_here\n</code></pre></p> </li> <li> <p>Run Work Journal:    <pre><code>work-journal\n# Use System menu (x) \u2192 Model configuration (1) for setup\n</code></pre></p> </li> <li> <p>Test Your Configuration:</p> </li> <li>Use \"Test Current Setup\" option to verify all models work</li> <li>Each provider is tested during setup</li> </ol>"},{"location":"providers/#advanced-tips","title":"Advanced Tips","text":"<p>Cost Optimization: Use local models for high-frequency operations (conversation, JIRA matching) and cloud models for quality-critical tasks (processing).</p> <p>Performance: Local models respond faster but cloud models generally produce higher quality results.</p> <p>Privacy: Keep sensitive operations (JIRA matching, personal notes) on local models.</p> <p>Reliability: Configure multiple providers as fallbacks for critical workflows.</p>"},{"location":"providers/#troubleshooting","title":"Troubleshooting","text":"<p>Local Model Issues: - Ensure Ollama is running: <code>ollama list</code> - Check LM Studio server is started on correct port - Verify model is downloaded: <code>ollama pull model-name</code></p> <p>API Key Issues: - Check environment variables: <code>echo $OPENAI_API_KEY</code> - Verify <code>.env</code> file location and format - Test with a simple API call outside Work Journal</p> <p>Connection Failures: - Use the built-in connection test in model configuration - Check firewall/proxy settings for API access - Verify local services are running on expected ports</p> <p>For configuration switching and management, see Configuration Management.</p>"}]}